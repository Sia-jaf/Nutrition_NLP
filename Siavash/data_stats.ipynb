{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ef56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, gzip, pyarrow.parquet\n",
    "\n",
    "Local_google_drive = \"/Users/siavash/Library/CloudStorage/GoogleDrive-siavash.jafarizadeh@gmail.com/.shortcut-targets-by-id/1BA94HYNI6NLWOcU5Ts7Nfso4uuVjrDwp/deeplearning2026\"\n",
    "\n",
    "df = pd.read_parquet('FNDDSeverything.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab44208",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.io.parquet.get_engine('auto')) \n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcdb737",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = df.select_dtypes(exclude=\"str\")\n",
    "d= data.fillna(0, inplace=True)\n",
    "d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.mapper import make_mapper_pipeline\n",
    "from gtda.plotting import plot_static_mapper_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = StandardScaler().fit_transform(df)\n",
    "\n",
    "# Create mapper graph\n",
    "mapper_pipeline = make_mapper_pipeline(\n",
    "#    filter_func='l2norm',\n",
    "    cover='balanced',\n",
    "    clusterer='agglomerative',\n",
    "    n_jobs=40,\n",
    "    min_intersection=0.3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit and get graph\n",
    "graph = mapper_pipeline.fit_transform(df)\n",
    "\n",
    "\n",
    "# Visualize\n",
    "\n",
    "plot_static_mapper_graph(graph)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53154ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# giotto-tda mapper functions\n",
    "from gtda.mapper import (\n",
    "    CubicalCover,\n",
    "    make_mapper_pipeline,\n",
    "    Projection,\n",
    "    plot_interactive_mapper_graph,\n",
    "    ParallelClustering\n",
    ")\n",
    "\n",
    "# 1. Prepare your data\n",
    "# Assuming df is your existing DataFrame\n",
    "# We must ensure only numeric data is used for the TDA pipeline\n",
    "\n",
    "data = df.select_dtypes(exclude=\"str\")\n",
    "\n",
    "# IMPORTANT: TDA is distance-based, so scaling is mandatory\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# 2. Define the \"Lens\" (Filter Function)\n",
    "# We use PCA to reduce the data into 2 dimensions to guide the mapping\n",
    "filter_func = PCA(n_components=2)\n",
    "\n",
    "# 3. Define the \"Cover\"\n",
    "# n_intervals: how many slices to cut the data into\n",
    "# overlap_frac: how much the slices overlap (creates the edges)\n",
    "cover = CubicalCover(n_intervals=15, overlap_frac=0.3)\n",
    "\n",
    "# 4. Define the \"Clustering\" algorithm\n",
    "# For big data, we wrap the clusterer in ParallelClustering to use all CPU cores\n",
    "clusterer = ParallelClustering(\n",
    "    DBSCAN(eps=0.5, min_samples=5), \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 5. Build the Mapper Pipeline\n",
    "# 'keep_data=True' is required to color the graph by your original features later\n",
    "pipe = make_mapper_pipeline(\n",
    "    filter_func=filter_func,\n",
    "    cover=cover,\n",
    "    clusterer=clusterer,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 6. Fit and generate the graph\n",
    "# This calculates the nodes and edges\n",
    "graph = pipe.fit_transform(data_scaled)\n",
    "\n",
    "# 7. Visualize the result\n",
    "# 'color_variable' allows you to color the graph nodes by any column in your original df\n",
    "# 'node_color_statistic' can be np.mean or np.median\n",
    "fig = plot_interactive_mapper_graph(\n",
    "    pipe, \n",
    "    data_scaled, \n",
    "    color_data=df,         # Use the original DF for coloring labels\n",
    "    node_color_statistic=np.mean\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de7021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting Mapper pipeline...\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# giotto-tda imports\n",
    "from gtda.mapper import make_mapper_pipeline, plot_static_mapper_graph\n",
    "\n",
    "from gtda.mapper import CubicalCover\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "df = df.select_dtypes(exclude=\"str\")\n",
    "df = df.fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "# Standardize the data (important for Mapper)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# ============================================\n",
    "# 2. CREATE MAPPER PIPELINE\n",
    "# ============================================\n",
    "cover = CubicalCover(n_intervals=10, overlap_frac=0.3)\n",
    "clusterer = AgglomerativeClustering()\n",
    "\n",
    "mapper_pipeline = make_mapper_pipeline(\n",
    "    filter_func=PCA(n_components=10),\n",
    "    cover=cover,\n",
    "    clusterer=clusterer,\n",
    "    n_jobs=10,\n",
    "    min_intersection=12\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 3. FIT THE MAPPER AND GET THE GRAPH\n",
    "# ============================================\n",
    "print(\"\\nFitting Mapper pipeline...\")\n",
    "graph = mapper_pipeline.fit_transform(df_scaled)\n",
    "\n",
    "print(f\"Graph created successfully!\")\n",
    "print(f\"Number of nodes: {graph.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {graph.number_of_edges()}\")\n",
    "\n",
    "# ============================================\n",
    "# 4. VISUALIZE THE MAPPER GRAPH\n",
    "# ============================================\n",
    "print(\"\\nGenerating visualization...\")\n",
    "\n",
    "# Static visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plot_static_mapper_graph(graph, ax=ax)\n",
    "ax.set_title('Mapper Graph - Clustering Visualization', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 5. ANALYZE THE GRAPH\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MAPPER GRAPH ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Node information\n",
    "print(f\"\\nNumber of clusters (nodes): {graph.number_of_nodes()}\")\n",
    "print(f\"Number of connections (edges): {graph.number_of_edges()}\")\n",
    "\n",
    "# Node sizes (data points per cluster)\n",
    "node_sizes = [graph.nodes[node]['node_elements'] for node in graph.nodes()]\n",
    "print(f\"\\nCluster sizes (data points per node):\")\n",
    "print(f\"  Min: {min(node_sizes)}\")\n",
    "print(f\"  Max: {max(node_sizes)}\")\n",
    "print(f\"  Mean: {np.mean(node_sizes):.2f}\")\n",
    "\n",
    "# Degree analysis\n",
    "degrees = [graph.degree(node) for node in graph.nodes()]\n",
    "print(f\"\\nNode connectivity (degree):\")\n",
    "print(f\"  Min: {min(degrees)}\")\n",
    "print(f\"  Max: {max(degrees)}\")\n",
    "print(f\"  Mean: {np.mean(degrees):.2f}\")\n",
    "\n",
    "# ============================================\n",
    "# 6. EXTRACT CLUSTER ASSIGNMENTS (OPTIONAL)\n",
    "# ============================================\n",
    "# Get node membership for each data point\n",
    "node_membership = mapper_pipeline.named_steps['mapper'].map_\n",
    "\n",
    "print(f\"\\nCluster assignments per data point:\")\n",
    "print(f\"Unique clusters: {len(np.unique(node_membership))}\")\n",
    "print(f\"Sample assignments (first 20): {node_membership[:20]}\")\n",
    "\n",
    "# Create a dataframe with original data and cluster assignment\n",
    "df_with_clusters = df.copy()\n",
    "df_with_clusters['mapper_cluster'] = node_membership\n",
    "\n",
    "print(f\"\\nData with cluster assignments:\\n{df_with_clusters.head(10)}\")\n",
    "\n",
    "# ============================================\n",
    "# 7. ADDITIONAL ANALYSIS: CLUSTER CHARACTERISTICS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLUSTER CHARACTERISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cluster_stats = df_with_clusters.groupby('mapper_cluster').agg(['count', 'mean', 'std'])\n",
    "print(f\"\\nStatistics per cluster:\\n{cluster_stats}\")\n",
    "\n",
    "# ============================================\n",
    "# 8. EXPERIMENT WITH DIFFERENT PARAMETERS (OPTIONAL)\n",
    "# ============================================\n",
    "# Uncomment to test different configurations\n",
    "\n",
    "configurations = [\n",
    "    {'filter_func': 'eccentricity', 'cover': 'balanced', 'n_intervals': 8},\n",
    "    {'filter_func': 'l2norm', 'cover': 'balanced', 'n_intervals': 12},\n",
    "    {'filter_func': 'projection', 'cover': 'uniform', 'n_intervals': 10},\n",
    "]\n",
    "\n",
    "# for config in configurations:\n",
    "#     print(f\"\\nTesting configuration: {config}\")\n",
    "#     mapper_test = make_mapper_pipeline(**config)\n",
    "#     graph_test = mapper_test.fit_transform(df_scaled)\n",
    "#     print(f\"  Nodes: {graph_test.number_of_nodes()}, Edges: {graph_test.number_of_edges()}\")\n",
    "\n",
    "print(\"\\n Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456f0d0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
